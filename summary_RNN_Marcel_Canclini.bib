@inproceedings{gers_recurrent_2000,
    title = {Recurrent Nets that Time and Count},
    volume = {3},
    doi = {10.1109/IJCNN.2000.861302},
    abstract = {The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by ``peephole connections'' from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count},
    timestamp = {2016-10-03T07:45:32Z},
    booktitle = {{{IJCNN}} 2000, {{Proceedings}} of the {{IEEE}}-{{INNS}}-{{ENNS International Joint Conference}} on {{Neural Networks}}, 2000},
    author = {Gers, F. A. and Schmidhuber, J.},
    year = {2000},
    keywords = {counting,counting circuits,Delay,discrete time steps,Event detection,Hidden Markov models,highly-nonlinear precisely-timed spike sequences,Humans,internal cells,learning (artificial intelligence),long short-term memory,LSTM,motor control,Motor drives,multiplicative gates,Pattern recognition,peephole connections,Performance loss,recurrent neural nets,Recurrent neural networks,Rhythm,rhythm detection,RNN,sequences,sequential tasks,stable sequences,time intervals,timing,World Wide Web},
    pages = {189--194 vol.3},
    file = {IEEE Xplore Abstract Record:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/8BD9EGXW/861302.html:text/html},
}

@article{hochreiter_long_1997,
    title = {Long {{Short}}-{{Term Memory}}},
    volume = {9},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    timestamp = {2016-10-02T18:28:12Z},
    number = {8},
    journal = {Neural Computation},
    author = {Hochreiter, S. and Schmidhuber, J.},
    year = {1997},
    pages = {1735--1780},
    file = {IEEE Xplore Abstract Record:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/CREB4XPG/6795963.html:text/html},
}

@article{lipton_critical_2015,
    title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
    abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
    timestamp = {2016-10-02T18:26:35Z},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1506.00019},
    primaryClass = {cs},
    urldate = {2016-10-02},
    journal = {arXiv:1506.00019 [cs]},
    author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
    year = {2015},
    keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
    file = {arXiv\:1506.00019 PDF:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/ITIEAMNI/Lipton et al. - 2015 - A Critical Review of Recurrent Neural Networks for.pdf:application/pdf;arXiv.org Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/4J4J45PM/1506.html:text/html},
}

@article{schuster_bidirectional_1997,
    title = {Bidirectional Recurrent Neural Networks},
    volume = {45},
    issn = {1053-587X},
    doi = {10.1109/78.650093},
    abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
    timestamp = {2016-10-03T07:33:41Z},
    number = {11},
    journal = {IEEE Transactions on Signal Processing},
    author = {Schuster, M. and Paliwal, K. K.},
    year = {1997},
    keywords = {artificial data,Artificial neural networks,bidirectional recurrent neural networks,classification experiments,complete symbol sequences,conditional posterior probability,Control systems,Databases,learning by example,learning from examples,negative time direction,Parameter estimation,pattern classification,phonemes,positive time direction,Probability,real data,recurrent neural nets,Recurrent neural networks,regression experiments,regular recurrent neural network,Shape,speech processing,speech recognition,statistical analysis,Telecommunication control,TIMIT database,training,Training data},
    pages = {2673--2681},
    file = {IEEE Xplore Abstract Record:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/WW6RCH47/650093.html:text/html},
}

